
#' Calls glmnet to fit propensity score models corresponding to different degrees of regularization
#'
#' @param data a dataset or matrix containing baseline covariates
#' @param treatment a binary vector for treatment
#' @param foldid fold each subject belongs to
#' @param alpha the elasticnet tuning parameter defined in glmnet (default is 1 for Lasso)
#' @param lambda_ratio the ratio from the largest to smallest lambda to consider (constrains the range of lambda values)
#' @param nlambda the number of lambda tuning parameters to consider when fitting glmnet
#' @param maxit maximum number of iterations as defined in glmnet
#' @param nmodels the number of undersmoothed models to return PS values for
#' @param penalty optional vector to specify different penalties for variables like in adaptive lasso (default is NULL)
#' @param par optional TRUE/FALSE to implement parallel computing (default is FALSE)
#'
#' @returns A list containing: 1) same-sample predicted values from the selected lasso model; 2) out-of-fold predicted values from the selected lasso; 3) coefficient matrix for all fitted lasso models; 4) lambda values for all fitted lasso models; 5) number of selected variables from each fitted lasso model.
#' @details The treatment_model() function fits LASSO propensity score models for causal inference in high-dimensional data. The function calls glmnet to fit several Lasso models with different lambda tuning parameters (using default sequence in glmnet).
#' 
#' To determine the degree of undersmoothing for each model, the function treatment_model() considers the default lambda sequence generated by glmnet(). This sequence is determined by lambda_ratio. The lambda_ratio is the ratio of the largest to smallest lambda values considered, and where the largest lambda value is the smallest value of lambda such that all coefficients are zero. By default, lambda_ratio is set to the default settings in glmnet which is 0.01 if the number of covariates exceed the sample size and 0.0001 if the number of covariates is less than the sample size. 
#' 
#' The function cv.glmnet automatically finds the lambda value that optimizes out-of-sample prediction using cross-validation. The function 'treatment_model' then begins with this lambda value (the one optimizing predictive performance) and allows the user to specify the number of lambda values to consider between this lambda value and largest lambda value in the sequence for fitting undersmoothed models (specified in the parameter 'nmodels'). The advantage of this approach is that it allows the user to consider a wide range of lambda values for undersmoothing and improves computation time by allowing the user to specify just a subset of models within this lambda range. The disadvantage is that it can undersmooth too quickly. Alternatives can include specifying the rate of undersmoothing as proposed in Ertefaie and van der Laan (2022), which we do not include here. 
#' 
#' Once the sequence of lambda values has been generated, a separate lasso model is fit for each lambda value. Note that treatment_model() does not perform model selection of the fitted models. This is done in a separate function ps_undersmooth_bal(). The predicted values from each lasso model are then given as input to ps_undersmooth_bal(), which then selects the model(s)/predicted values using the specified selection method for undersmoothing within this function. Options available for determining the degree of undersmoothing include 1) balance optimization (for PS weighting), See function ps_undersmooth_bal() for details. 
#' @export
#' @examples
#' #loading package
#' library(PSLassoSynthNC)
#' 
#' #creating some simulated data for testing
#' nstudy<- 2000
#' nvars<- 500
#' nc<- 100
#' ns<- nvars-(nc)
#' alpha_temp<- runif(nc, 0.0, 0.4)
#' beta_temp<- runif(nc, 0.0, 0.4)
#' random_neg<- sample(1:length(alpha_temp), 0.5*length(alpha_temp), replace=FALSE)
#' alpha_temp[random_neg]<- -1*alpha_temp[random_neg]
#' beta_temp[random_neg]<-  -1*beta_temp[random_neg]
#' alpha<-  matrix(c(alpha_temp, rep(0, ns)), ncol=1)
#' beta<-   matrix(c(beta_temp, rep(0, ns)), ncol=1)
#' betaE<- 0
#' cprev<- runif(nvars, 0, 0.3)
#' cprev<- sample(cprev)
#' oprev<- 0.05
#' tprev<- 0.4
#' Xcovs_sim<- matrix(rnorm((nstudy*nvars), 0, 1), nrow=nstudy, ncol=nvars)
#' Xcovs_sim<- as.data.frame(Xcovs_sim)  
#' names(Xcovs_sim)<- c(paste0('x', 1:nvars))
#' W<- as.matrix(Xcovs_sim)
#' colnames(W)<- c(paste0('x', 1:nvars))
#' linear_pred_e<- W %*% alpha
#' linear_pred_y<- W %*% beta
#' treatment_inc<- tprev
#' fn <- function(c) mean(plogis(c + linear_pred_e)) - treatment_inc
#' alpha0 <- uniroot(fn, lower = -20, upper = 20)$root
#' Ee <- (1 + exp( -(alpha0 + linear_pred_e) ))^-1
#' e<- rbinom(nstudy, 1, Ee)
#' outcome_inc<- oprev
#' fn <- function(c) mean(plogis(c + betaE*e + linear_pred_y  )) - outcome_inc
#' beta0 <- uniroot(fn, lower = -20, upper = 20)$root
#' Ey <- (1 + exp( -( beta0 + betaE*e + linear_pred_y )))^-1
#' y<- rbinom(nstudy, 1, Ey)
#' simdat <- as.data.frame(cbind(y, e, Ee, Xcovs_sim))
#'
#' #creating folid vector for testing
#' N <- length(e)
#' V=10
#' n<- 1:length(e)
#' cvfolds<- stratifyCVFoldsByYandID(V=V, Y = e)
#' folds <- cvfolds$validRows
#' foldid <- cvfolds$fold_id
#'
#' #running treatment_model function
#' treatment_model(data=Xcovs_sim, treatment=e, foldid=foldid, alpha=1, lambda_ratio=NULL, nlambda=100, nmodels=NULL, maxit=5000, penalty=NULL, par=FALSE)
treatment_model<- function(data,
                           treatment,
                           foldid,
                           alpha=1,
                           lambda_ratio=NULL,
                           nlambda=100,
                           nmodels=NULL,
                           maxit=5000,
                           penalty=NULL,
                           par=FALSE){
  Wmat = as.matrix(data)
  sx = Matrix::Matrix(Wmat, sparse=TRUE)
  if(is.null(penalty)){
    penalty_factor<- rep(1, ncol(Wmat))
  }
  if(!is.null(penalty)){
    penalty_factor<- penalty
  }
  if(is.null(lambda_ratio)){
    lambda_ratio<- ifelse(nrow(sx) < ncol(sx), 0.01, 1e-04)
  }
  glmnet.e<- NULL
  glmnet.e<- glmnet::cv.glmnet(x = sx,
                               y = treatment,
                               family = "binomial",
                               type.measure = "deviance",
                               alpha = alpha, ##ridge regression alpha=0, lasso alpha=1
                               nlambda = nlambda,
                               lambda.min.ratio = lambda_ratio, #ifelse(nrow(sx) < ncol(sx), 0.01, 1e-04),
                               #nfolds = 10, ##donâ€™t specify this when using foldid
                               #penalty.factor = c(rep(1, dim(sx)[2])),
                               penalty.factor = penalty_factor,
                               parallel = par,
                               standardize = FALSE, ###HAL does not standardize design matrix
                               maxit=maxit, #5000,
                               foldid = foldid,
                               keep=TRUE)
  
  ## extracting predicted values & lambda starting and stopping values
  gns1<- NULL
  gns2<- NULL
  gns1<- predict(glmnet.e, newx = Wmat, s=glmnet.e$lambda, type = "response") ##predicted values for each lambda
  gns2<- plogis(glmnet.e$fit.preval[, 1:ncol(gns1)]) ## Cross Validated (out-of-fold) predicted values for each lambda
  
  ## lambda value that optimizes CV prediction (minimizes CV prediction error)
  n.lambda.start<- which(glmnet.e$lambda == glmnet.e$lambda.min)
  lambda.start<- glmnet.e$lambda.min
  
  ## only keeping lambda values that are less than or equal to lambda that optimizes CV prediction
  preds_under1<- gns1[,n.lambda.start:ncol(gns1)] ##same-sample predicted values
  preds_under2<- gns2[,n.lambda.start:ncol(gns2)] ##out-of-fold predicted values
  lambda_vector<- glmnet.e$lambda[n.lambda.start:length(glmnet.e$lambda)]
  lassocoef = glmnet.e$glmnet.fit$beta[,n.lambda.start:length(glmnet.e$lambda)]
  coef_mat<- lassocoef
  
  ##number of selected variables for each lambda value
  n_selected_vars<- apply(lassocoef, 2, function(x){sum(x != 0)})
  
  results<- list(preds_under1,
                 preds_under2,
                 coef_mat,
                 lambda_vector,
                 n_selected_vars)
  
  names(results)<- c("preds",
                     "preds_cf",
                     "coef_mat",
                     "lambdas",
                     "n_vars")
  return(results)
}